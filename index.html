<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <!-- SEO and Metadata -->
    <title>DH-OmniFace: A Large-Scale and Multi-Attribute Dataset Suite for Controllable Face Video Generation    </title>
    <meta name="description" content="DH-OmniFace is a large-scale suite of human-centric video datasets covering diverse facial attributes and full-body actions. It contains 340K clips (1,879 hours) with paired speech audio and text annotations. The dataset features six core emotions, 16 lighting conditions, 360° head poses, and dynamic actions, enabling controllable and high-quality face and body video generation.">
    <meta name="keywords" content="DH-OmniFace, Human-Centric Video Dataset, Controllable Video Generation, Facial Emotion Dataset, 360° Head Pose, Lighting Variation, Full-Body Actions, Multimodal Learning, Generative AI, Fine-Grained Attribute Control, U-Net, DiT, Autoregressive Models, FACET Benchmark, High-Resolution Video Synthesis, Project Page, 2025">
    
    <!-- External Libraries -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    
    <!-- Consolidated CSS -->
    <style>
      body {
        font-family: 'Roboto', sans-serif;
        line-height: 1.7;
        color: #333;
      }
      .container {
          max-width: 1320px;
      }

      /* --- NEW: Header & Logo Side-by-Side Styling --- */
      .title-header {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 25px; /* Space between logo and text */
        margin-bottom: 2rem;
      }
      .logo-container svg {
        width: 100px; /* Adjusted size for side-by-side layout */
        height: auto;
        border-radius: 50%;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
      }
      .title-text-block {
        text-align: left; /* Align text to the left in this block */
      }
      h1 { font-weight: 700; font-size: 2.5rem; margin-bottom: 0.25rem; }
      .publication-venue {
        font-size: 1.3rem;
        font-weight: 500;
        color: #28a745;
        margin-bottom: 0;
      }
      /* --- End Header Styling --- */
      
      .author-line, .institution-line { font-size: 1.1rem; line-height: 1.8; }
      .author-block, .inst-block { display: inline-block; margin: 0 0.5rem; }
      .inst-block { color: #555; }
      
      .section-title, .title { 
        font-size: 2rem; font-weight: 700; 
        border-bottom: 2px solid #eee; padding-bottom: 0.5rem; 
        margin-bottom: 2rem;
      }
      .section-subtitle { font-size: 1.25rem; font-weight: 500; color: #666; margin-top: 2rem; }
      .section-text { text-align: justify; }

      .abstract-text { font-style: italic; }
      .tldr { font-size: 1.1em; margin-top: 1.5rem; padding: 1rem; background-color: #f8f9fa; border-left: 4px solid #007bff; }

      .description {
        font-size: 1.1rem;
        color: #6c757d;
        margin-top: 1rem;
      }
      .btn-download-specific {
        display: inline-block; font-weight: 500; color: #fff; background-color: #28a745;
        border-color: #28a745; padding: 0.75rem 1.5rem; font-size: 1.25rem;
        line-height: 1.5; border-radius: 0.3rem; text-decoration: none;
        transition: all .15s ease-in-out; margin-top: 1.5rem;
      }
      .btn-download-specific:hover {
        color: #fff; background-color: #218838; border-color: #1e7e34; text-decoration: none;
      }
      .download-alert { background-color: #fff8e1; border: 1px solid #ffecb3; padding: 1.5rem; border-radius: 8px; text-align: left; }
      .download-alert h3 { margin-top: 0; color: #c09300; }

      @keyframes fadeInUp {
        from { opacity: 0; transform: translateY(20px); }
        to   { opacity: 1; transform: translateY(0); }
      }
      .timeline { 
        position: relative; padding-left: 50px; list-style: none;
      }
      .timeline::before { 
        content: ''; position: absolute; top: 0; left: 18px; 
        height: 100%; width: 4px; background: #e9ecef; border-radius: 2px; 
      }
      .timeline-item { 
        position: relative; margin-bottom: 40px; 
        animation: fadeInUp 0.5s ease-out forwards;
        opacity: 0;
      }
      .timeline-item:nth-child(1) { animation-delay: 0.1s; }
      .timeline-item:nth-child(2) { animation-delay: 0.2s; }
      .timeline-item:nth-child(3) { animation-delay: 0.3s; }
      .timeline-item:nth-child(4) { animation-delay: 0.4s; }

      .timeline-item.status-current::after {
        content: ''; position: absolute; top: 18px; left: 18px;
        width: 4px; height: calc(100% + 40px); background-color: #007bff; z-index: 0;
      }
      .timeline-item:first-child.status-current::after { top: 0; }
      
      .timeline-dot { 
        position: absolute; left: 0; top: 0; transform: translateX(-50%); 
        width: 36px; height: 36px; border-radius: 50%; background: #fff; 
        border: 4px solid; z-index: 1; display: flex;
        justify-content: center; align-items: center; font-size: 1.1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
      }
      .timeline-content { padding-left: 30px; }
      .timeline-content h3 { 
        font-size: 1.4em; font-weight: 600; color: #343a40; 
        margin: 0 0 8px 0; display: flex; align-items: center;
      }
      .timeline-content p { margin: 0; color: #495057; }
      .timeline-status {
        font-size: 0.6em; font-weight: 700; padding: 4px 10px;
        border-radius: 12px; margin-left: 12px;
      }
      
      .timeline-item.status-current .timeline-dot { border-color: #007bff; background-color: #007bff; color: white; }
      .timeline-item.status-current .timeline-status { background-color: #e6f2ff; color: #007bff; }
      .timeline-item.status-current p { color: #0056b3; font-weight: 500; }

      .timeline-item.status-future { opacity: 0.7; }
      .timeline-item.status-future .timeline-dot { border-color: #6c757d; background-color: #fff; color: #6c757d; }
      .timeline-item.status-future .timeline-status { background-color: #f8f9fa; color: #6c757d; }
      .timeline-item.status-future h3, .timeline-item.status-future p { color: #6c757d; }

      .figure-img { max-width: 80%; display: block; margin: 1rem auto; }

      .video-gallery {
        display: grid; grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 20px; background-color: #f8f9fa; padding: 20px; border-radius: 8px;
      }
      .video-wrapper {
        position: relative; width: 100%; padding-top: 100%;
        border-radius: 8px; overflow: hidden;
        box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        cursor: pointer; transition: transform 0.2s ease;
      }
      .video-wrapper:hover { transform: scale(1.05); z-index: 10; }
      .video-wrapper video { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; }
      .video-title {
        position: absolute; bottom: 0; left: 0; right: 0;
        background: rgba(0,0,0,0.6); color: white;
        padding: 5px; font-size: 12px; text-align: center;
        white-space: nowrap; overflow: hidden; text-overflow: ellipsis;
        opacity: 0; transition: opacity 0.2s ease;
      }
      .video-wrapper:hover .video-title { opacity: 1; }

      .bibtex-box pre { background-color: #f8f9fa; padding: 1.5rem; border-radius: 5px; white-space: pre-wrap; word-break: break-all; }
      
      @media (max-width: 768px) {
        /* Responsive: Stack logo and title on mobile */
        .title-header {
          flex-direction: column;
          gap: 15px;
        }
        .title-text-block {
          text-align: center;
        }
        h1 { font-size: 2rem; }
        .publication-venue { font-size: 1.1rem; }
        .figure-img { max-width: 100%; }
        .video-gallery { grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); }
        .timeline { padding-left: 30px; }
        .timeline-content { padding-left: 20px; }
      }
    </style>
</head>

<body>
  <div class="container my-4">
    
    <!-- Header -->
    <div class="text-center">
      <!-- LOGO & TITLE SIDE-BY-SIDE -->
      <div class="title-header">
        <div class="logo-container">
          <svg width="200" height="200" viewBox="0 0 200 200" xmlns="http://www.w3.org/2000/svg">
            <defs>
              <clipPath id="face-clip-b">
                <path d="M 100,20 C 60,20 30,50 30,100 C 30,170 70,190 100,190 C 130,190 170,170 170,100 C 170,50 140,20 100,20 Z" />
              </clipPath>
            </defs>
            <g clip-path="url(#face-clip-b)">
              <polygon points="30,20 100,100 30,120" fill="#003f5c"/>
              <polygon points="30,20 100,20 100,100" fill="#2f4b7c"/>
              <polygon points="100,20 170,20 100,100" fill="#665191"/>
              <polygon points="170,20 170,120 100,100" fill="#a05195"/>
              <polygon points="30,120 100,100 100,190 30,190" fill="#d45087"/>
              <polygon points="100,100 170,120 170,190 100,190" fill="#f95d6a"/>
              <polygon points="100,100 100,190 130,150" fill="#ff7c43"/>
              <polygon points="100,100 70,150 100,190" fill="#ffa600"/>
            </g>
            <circle cx="75" cy="90" r="12" fill="#ffffff"/>
            <polygon points="71,84 82,90 71,96" fill="#003f5c"/>
            <circle cx="125" cy="90" r="8" fill="#ffffff" />
            <circle cx="125"cy="90" r="4" fill="#003f5c" />
            <path d="M 90,140 Q 100,150 110,140" stroke="#ffffff" stroke-width="4" fill="none" stroke-linecap="round"/>
          </svg>
        </div>
        <div class="title-text-block">
          <h1>DH-OmniFace: A Comprehensive Dataset Suite for Controllable Human-Centric Video Generation</h1>
          <h3 class="publication-venue">Arxiv 2025</h3>
        </div>
      </div>
      
      <div class="author-line">
        <span class="author-block"><a href="https://github.com/fenghe12">He Feng</a><sup>1</sup>,</span>
        <span class="author-block"><a href="https://scholar.google.com/citations?hl=zh-CN&user=L8tcNioAAAAJ">Donglin Di</a><sup>1</sup>,</span>
        <span class="author-block"><a href="https://scholar.google.hk/citations?hl=zh-CN&user=67fxVzoAAAAJ">Tonghua Su</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Yin Chen</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Xiu Su</a><sup>2</sup>,</span>
        <span class="author-block"><a href="#">Hongyan Xu</a><sup>2</sup>,</span>
        <span class="author-block"><a href="#">Zhongjie Wang</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Xiangqian Wu</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Song Yang</a><sup>3</sup>,</span>
        <span class="author-block"><a href="https://hellodfan.github.io/">Lei Fan</a><sup>3</sup></span>
      </div>
      
      <div class="institution-line mt-2">
        <span class="inst-block"><sup>1</sup>Harbin Institute of Technology</span>
        <span class="inst-block"><sup>2</sup>Central South University</span>
        <span class="inst-block"><sup>3</sup>University of New South Wales</span>
      </div>
      
      <div class="mt-4">
        <a class="btn btn-primary btn-lg m-2" href="#" role="button">Paper</a>
        <a class="btn btn-secondary btn-lg m-2" href="javascript:void(0);" role="button">Video</a>
        <a class="btn btn-secondary btn-lg m-2" href="javascript:void(0);" role="button">Poster</a>
        <a class="btn btn-success btn-lg m-2" href="https://docs.google.com/forms/d/e/1FAIpQLSeSMkATTXO22YvLFj-qC_hDM8LCd912Y45cdyINU91HbgP9KQ/viewform?usp=header" role="button">Data</a>
      </div>

    <!-- 1. MERGED Abstract & Overview Section -->
    <div class="my-5">
      <h2 class="section-title">📜 Overview & Abstract</h2>
      <img src="static/images/figure1-0918.png" class="img-fluid rounded figure-img" alt="Overview of the DH-OmniFace dataset">
      <p class="section-text mt-3">
         Overview of the <strong>DH-OmniFace</strong>. The suite includes seven components: <strong>DH-FaceVid-1K</strong> for talking face generation, <strong>DH-FaceEmoVid</strong> for expressive emotions, <strong>DH-FaceReliVid</strong> for diverse lighting conditions, <strong>DH-FaceLolVid</strong> for authentic laughter, <strong>DH-FaceDrasMvVid</strong> for large-angle head movements, <strong>DH-MVHeadVid</strong> for multi-view head poses, and <strong>DH-SingleVid</strong> for complex body movements.  
        The lower panels summarize our experiments: (i) fine-tuning generative backbones, (ii) introducing the FACET benchmark, and (iii) exploring joint training of face and full-body data in full-body video generation.
      </p>
      <p class="abstract-text section-text mt-4">
        Human-centric video generation has made significant progress in synthesizing realistic, temporally coherent videos. However, advancing controllable generation of fine-grained facial attributes, <em>e.g.</em>, emotion, head pose, and lighting conditions, is hindered by the lack of large-scale, high-quality datasets with rich annotations.  
        To address this gap, we present <strong>DH-OmniFace</strong>, a comprehensive suite of human-centric video datasets covering diverse facial attributes and full-body actions.  
        The suite comprises <strong>340,407</strong> video clips totaling <strong>1,879 hours</strong> from over <strong>20,000</strong> individuals, including <strong>128,392</strong> clips at <code>1080×1080</code> and <strong>23,887</strong> clips at 1080p.  
        To ensure quality, we establish a multi-stage pipeline to filter out noisy samples, such as hands appearing at frame edges and truncated logos, which could otherwise degrade generation quality.  
        Each sample is paired with synchronized speech audio and text annotations to support multimodal learning.  
        The dataset offers extensive attribute coverage, including six core emotions (<em>e.g.</em>, happy, angry, disgusted), 16 lighting conditions (artificial and natural light at different times of day), 360° head poses (captured by a 12-camera array covering frontal, profile, and back views), and dynamic actions (including laughter and complex body movements such as calisthenics and yoga).  
      </p>
      <div class="tldr">
        <strong>TL;DR:</strong> We introduce <strong>DH-OmniFace</strong>, a large-scale suite of human-centric video datasets with diverse facial attributes and full-body actions, enabling controllable and high-quality video generation across multiple modalities.
      </div>
    </div>
    
    <div class="my-5">
      <div class="text-center">
        <h2 class="title">📥 Download DH-OmniFace</h2>
        <a href="https://docs.google.com/forms/d/e/1FAIpQLSeSMkATTXO22YvLFj-qC_hDM8LCd912Y45cdyINU91HbgP9KQ/viewform?usp=header" class="btn-download-specific">Click me for application</a>
        <div class="description mt-2"><strong>Scale:</strong> 340k samples / 1.8k hrs duration / ~5 TB</div>
      </div>
    
      <div class="download-alert mt-4">
        <h3>🔥 Download Instructions & Policy</h3>
        <p>To download the <strong>DH-OmniFace</strong> dataset suite, please submit the <a href="https://docs.google.com/forms/d/e/1FAIpQLSeSMkATTXO22YvLFj-qC_hDM8LCd912Y45cdyINU91HbgP9KQ/viewform?usp=header"><strong>request form</strong></a> using an official institutional email and clearly stating your research purpose. Requests from personal emails will be rejected.</p>
        <p>After approval (usually within <strong>2–3 working days</strong>), download instructions will be sent via email. If you encounter any issues, contact <a href="mailto:fenghe021209@gmail.com"><strong>fenghe021209@gmail.com</strong></a>.</p>
        <p>These samples are sourced from crowd-sourcing platforms. By downloading, you agree to comply with <a href="https://github.com/fenghe12/DH-OmniFace/blob/main/LICENSE"><strong>the license agreement</strong></a>.</p>
      </div>
    </div>
    
    <!-- 3. Open-source Plan -->
<div class="my-5">
  <h2 class="section-title">🚀 Open-source Plan</h2>
  <p class="text-center">Our open-source roadmap is ongoing. We will update the status here as we make progress.</p>
  <div class="timeline mt-5">
    <div class="timeline-item status-current">
      <div class="timeline-dot">🔄</div>
      <div class="timeline-content">
        <h3>Phase 1 <span class="timeline-status">Ongoing</span></h3>
        <p>Open the DH-OmniFace dataset suite in a controlled manner for public access.</p>
      </div>
    </div>
  </div>
</div

    <!-- 4. Sampled Videos -->
    <div class="my-5">
      <h2 class="section-title">🎬 Sampled Videos</h2>
      <p class="text-center">Please note that to ensure smooth page loading, we have resized all videos to 256x256.</p>
      <h4 class="section-subtitle">Diverse and high-quality Asian face videos.</h4>
      <div id="asian-gallery" class="video-gallery mt-3"></div>
      <h4 class="section-subtitle">Multi-ethnic face videos.</h4>
      <div id="race-gallery" class="video-gallery mt-3"></div>
      <h4 class="section-subtitle">Face videos covering a wide range of age distributions.</h4>
      <div id="age-gallery" class="video-gallery mt-3"></div>
      <h4 class="section-subtitle">Face videos covering various head poses.</h4>
      <div id="pose-gallery" class="video-gallery mt-3"></div>
      <h4 class="section-subtitle">Face videos covering various emotions.</h4>
      <div id="emotion-gallery" class="video-gallery mt-3"></div>
    </div>

    <!-- 5. Statistics and Other Figures -->
    <div class="my-5 text-center">
      <h2 class="section-title">📊 Datasets Comparison</h2>
      <p class="section-text">
        Compared with other datasets, <strong>DH-OmniFace</strong> has a larger data volume, superior visual quality, and broader attribute coverage.
      </p>
      <img src="static/images/img_v3_02r4_88423c55-6652-4cf9-89b0-a19261c1336g.jpg" class="img-fluid rounded figure-img" alt="Comparison with other datasets">
    </div>
    
    <div class="my-5 text-center">
      <h2 class="section-title">📈 Statistics</h2>
      <p class="section-text">
        Details of each subset.
      </p>
      <img src="static/images/img_v3_02r4_88423c55-6652-4cf9-89b0-a19261c1336g.jpg" class="img-fluid rounded figure-img" alt="Dataset statistics">
    </div>
    
    <div class="my-5 text-center">
      <h2 class="section-title">⚙️ Collection Pipeline</h2>
      <img src="static/images/figure4_cropped.png" class="img-fluid rounded figure-img" alt="Data collection pipeline">
      <p class="section-text mt-3">
        Overview of the data curation pipeline, comprising four main stages: raw data acquisition, video preprocessing, noisy data filtering, and annotation generation using Qwen2.5-VL and Llama 3, followed by two-stage manual verification.
      </p>
    </div>
    
    <!-- BibTeX -->
    <div class="my-5">
      <h2 class="section-title">📚 BibTeX</h2>
      Coming Soon......
      <!-- <div class="bibtex-box">
        <pre><code>@inproceedings{di2025facevid,
    title = {DH-FaceVid-1K: A Large-Scale High-Quality Dataset for Face Video Generation},
    author = {Di, Donglin and Feng, He and Sun, Wenzhang and Ma, Yongjia and Li, Hao and Chen, Wei and Fan, Lei and Su, Tonghua and Yang, Xun},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year = {2025}
}</code></pre> -->
      </div>
    </div>
    
    <!-- Footer -->
    <footer class="text-center my-5">
      <p>Homepage Template adapted from <a href="https://github.com/m-niemeyer/regnerf" target="_blank">the RegNeRF project page</a>.</p>
    </footer>

  </div>

  <!-- Consolidated JavaScript -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      function createVideoGrid(containerId, fileList, directory) {
        const gridContainer = document.getElementById(containerId);
        if (!gridContainer) return;

        fileList.forEach(filename => {
          const wrapper = document.createElement('div');
          wrapper.className = 'video-wrapper';

          const video = document.createElement('video');
          video.loop = true;
          video.muted = true;
          video.playsInline = true;
          video.preload = 'metadata';
          
          wrapper.addEventListener('mouseenter', () => video.play().catch(e => {}));
          wrapper.addEventListener('mouseleave', () => video.pause());

          const source = document.createElement('source');
          source.src = directory + filename;
          source.type = 'video/mp4';

          const title = document.createElement('div');
          title.className = 'video-title';
          title.textContent = filename.replace('.mp4', '');

          video.appendChild(source);
          wrapper.appendChild(video);
          wrapper.appendChild(title);
          gridContainer.appendChild(wrapper);
          
          wrapper.addEventListener('click', () => {
            if (video.paused) video.play().catch(e => {});
            else video.pause();
          });
        });
      }

      const asianFiles = ["000680.mp4", "001406.mp4", "001592.mp4", "001600.mp4", "002161.mp4", "002523.mp4", "002728.mp4", "003696.mp4", "005192.mp4", "007860.mp4", "007956.mp4", "024572.mp4", "067379.mp4", "085089.mp4", "091313.mp4", "092616.mp4", "105369.mp4", "106321.mp4", "RS127258_segment_049_7350_0.mp4", "RS127710_segment_005_745_0.mp4"];
      const raceFiles = ["026237.mp4", "046378.mp4", "061175.mp4", "076105.mp4", "39Br2A7lxac_22.mp4", "3lfO6OCqcCA_0.mp4", "7n619EfuSPw_0.mp4", "BFs-a-hqs2I_9.mp4", "QaFqZQ6JQhs_1.mp4", "tqSUS5-JXIs_1.mp4", "UF2c01_glHU_3.mp4", "Uu3xazfdmvk_34.mp4", "V4ZyJR30wyg_29.mp4", "WA1L8vXkSKQ_0001_S370_E849_L115_T107_R515_B507.mp4", "WN2XSI6vZIg_18.mp4", "XoGhKL6CKwg_12.mp4"];
      const ageFiles = ["001106.mp4", "002148.mp4", "003762.mp4", "019479.mp4", "020955.mp4", "028457.mp4", "029951.mp4", "039691.mp4", "44840.mp4", "47079.mp4"];
      const poseFiles = ["008001.mp4", "013064.mp4", "027862.mp4", "033011.mp4", "034785.mp4", "0s1UUn9aSSw_7.mp4", "3nYrako9XM4_0.mp4", "55025.mp4", "61005.mp4", "62149.mp4", "_W4Em_fHubY_10.mp4", "_zNhY-IBLzc_64.mp4"];
      const emotionFiles = ["14435.mp4", "15002.mp4", "Czb5Ml9VDsI_0.mp4", "GrjEDguF59Q_0.mp4", "hM3nn30NxCE_0.mp4", "PP9l4LP0WPI_0.mp4", "qfEkv726kdQ_6.mp4", "qnFWCagTOtw_1.mp4", "V4cpZlFESeA_87.mp4", "Z_9KUBCidow_3.mp4"];

      createVideoGrid('asian-gallery', asianFiles, "facevid/asian/");
      createVideoGrid('race-gallery', raceFiles, "facevid/race/");
      createVideoGrid('age-gallery', ageFiles, "facevid/age/");
      createVideoGrid('pose-gallery', poseFiles, "facevid/pose/");
      createVideoGrid('emotion-gallery', emotionFiles, "facevid/emotion/");
    });
  </script>
</body>
</html>
