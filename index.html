<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <!-- SEO and Metadata -->
    <title>DH-OmniFace: A Large-Scale and Multi-Attribute Dataset Suite for Controllable Face Video Generation    </title>
    <meta name="description" content="DH-OmniFace is a large-scale suite of human-centric video datasets covering diverse facial attributes and full-body actions. It contains 340K clips (1,879 hours) with paired speech audio and text annotations. The dataset features six core emotions, 16 lighting conditions, 360° head poses, and dynamic actions, enabling controllable and high-quality face and body video generation.">
    <meta name="keywords" content="DH-OmniFace, Human-Centric Video Dataset, Controllable Video Generation, Facial Emotion Dataset, 360° Head Pose, Lighting Variation, Full-Body Actions, Multimodal Learning, Generative AI, Fine-Grained Attribute Control, U-Net, DiT, Autoregressive Models, FACET Benchmark, High-Resolution Video Synthesis, Project Page, 2025">
    
    <!-- External Libraries -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    
    <!-- Consolidated CSS -->
    <style>
      body {
        font-family: 'Roboto', sans-serif;
        line-height: 1.7;
        color: #333;
      }
      .container {
          max-width: 1320px;
      }

      /* --- NEW: Header & Logo Side-by-Side Styling --- */
      .title-header {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 25px; /* Space between logo and text */
        margin-bottom: 2rem;
      }
      .logo-container svg {
        width: 100px; /* Adjusted size for side-by-side layout */
        height: auto;
        border-radius: 50%;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
      }
      .title-text-block {
        text-align: left; /* Align text to the left in this block */
      }
      h1 { font-weight: 700; font-size: 2.5rem; margin-bottom: 0.25rem; }
      .publication-venue {
        font-size: 1.3rem;
        font-weight: 500;
        color: #28a745;
        margin-bottom: 0;
      }
      /* --- End Header Styling --- */
      
      .author-line, .institution-line { font-size: 1.1rem; line-height: 1.8; }
      .author-block, .inst-block { display: inline-block; margin: 0 0.5rem; }
      .inst-block { color: #555; }
      
      .section-title, .title { 
        font-size: 2rem; font-weight: 700; 
        border-bottom: 2px solid #eee; padding-bottom: 0.5rem; 
        margin-bottom: 2rem;
      }
      .section-subtitle { font-size: 1.25rem; font-weight: 500; color: #666; margin-top: 2rem; }
      .section-text { text-align: justify; }

      .abstract-text { font-style: italic; }
      .tldr { font-size: 1.1em; margin-top: 1.5rem; padding: 1rem; background-color: #f8f9fa; border-left: 4px solid #007bff; }

      .description {
        font-size: 1.1rem;
        color: #6c757d;
        margin-top: 1rem;
      }
      .btn-download-specific {
        display: inline-block; font-weight: 500; color: #fff; background-color: #28a745;
        border-color: #28a745; padding: 0.75rem 1.5rem; font-size: 1.25rem;
        line-height: 1.5; border-radius: 0.3rem; text-decoration: none;
        transition: all .15s ease-in-out; margin-top: 1.5rem;
      }
      .btn-download-specific:hover {
        color: #fff; background-color: #218838; border-color: #1e7e34; text-decoration: none;
      }
      .download-alert { background-color: #fff8e1; border: 1px solid #ffecb3; padding: 1.5rem; border-radius: 8px; text-align: left; }
      .download-alert h3 { margin-top: 0; color: #c09300; }

      @keyframes fadeInUp {
        from { opacity: 0; transform: translateY(20px); }
        to   { opacity: 1; transform: translateY(0); }
      }
      .timeline { 
        position: relative; padding-left: 50px; list-style: none;
      }
      .timeline::before { 
        content: ''; position: absolute; top: 0; left: 18px; 
        height: 100%; width: 4px; background: #e9ecef; border-radius: 2px; 
      }
      .timeline-item { 
        position: relative; margin-bottom: 40px; 
        animation: fadeInUp 0.5s ease-out forwards;
        opacity: 0;
      }
      .timeline-item:nth-child(1) { animation-delay: 0.1s; }
      .timeline-item:nth-child(2) { animation-delay: 0.2s; }
      .timeline-item:nth-child(3) { animation-delay: 0.3s; }
      .timeline-item:nth-child(4) { animation-delay: 0.4s; }

      .timeline-item.status-current::after {
        content: ''; position: absolute; top: 18px; left: 18px;
        width: 4px; height: calc(100% + 40px); background-color: #007bff; z-index: 0;
      }
      .timeline-item:first-child.status-current::after { top: 0; }
      
      .timeline-dot { 
        position: absolute; left: 0; top: 0; transform: translateX(-50%); 
        width: 36px; height: 36px; border-radius: 50%; background: #fff; 
        border: 4px solid; z-index: 1; display: flex;
        justify-content: center; align-items: center; font-size: 1.1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
      }
      .timeline-content { padding-left: 30px; }
      .timeline-content h3 { 
        font-size: 1.4em; font-weight: 600; color: #343a40; 
        margin: 0 0 8px 0; display: flex; align-items: center;
      }
      .timeline-content p { margin: 0; color: #495057; }
      .timeline-status {
        font-size: 0.6em; font-weight: 700; padding: 4px 10px;
        border-radius: 12px; margin-left: 12px;
      }
      
      .timeline-item.status-current .timeline-dot { border-color: #007bff; background-color: #007bff; color: white; }
      .timeline-item.status-current .timeline-status { background-color: #e6f2ff; color: #007bff; }
      .timeline-item.status-current p { color: #0056b3; font-weight: 500; }

      .timeline-item.status-future { opacity: 0.7; }
      .timeline-item.status-future .timeline-dot { border-color: #6c757d; background-color: #fff; color: #6c757d; }
      .timeline-item.status-future .timeline-status { background-color: #f8f9fa; color: #6c757d; }
      .timeline-item.status-future h3, .timeline-item.status-future p { color: #6c757d; }

      .figure-img { max-width: 80%; display: block; margin: 1rem auto; }

      .video-gallery {
        display: grid; grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 20px; background-color: #f8f9fa; padding: 20px; border-radius: 8px;
      }
      .video-wrapper {
        position: relative; width: 100%; padding-top: 100%;
        border-radius: 8px; overflow: hidden;
        box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        cursor: pointer; transition: transform 0.2s ease;
      }
      .video-wrapper:hover { transform: scale(1.05); z-index: 10; }
      .video-wrapper video { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; }
      .video-title {
        position: absolute; bottom: 0; left: 0; right: 0;
        background: rgba(0,0,0,0.6); color: white;
        padding: 5px; font-size: 12px; text-align: center;
        white-space: nowrap; overflow: hidden; text-overflow: ellipsis;
        opacity: 0; transition: opacity 0.2s ease;
      }
      .video-wrapper:hover .video-title { opacity: 1; }

      .bibtex-box pre { background-color: #f8f9fa; padding: 1.5rem; border-radius: 5px; white-space: pre-wrap; word-break: break-all; }
      
      @media (max-width: 768px) {
        /* Responsive: Stack logo and title on mobile */
        .title-header {
          flex-direction: column;
          gap: 15px;
        }
        .title-text-block {
          text-align: center;
        }
        h1 { font-size: 2rem; }
        .publication-venue { font-size: 1.1rem; }
        .figure-img { max-width: 100%; }
        .video-gallery { grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); }
        .timeline { padding-left: 30px; }
        .timeline-content { padding-left: 20px; }
      }
    </style>
</head>

<body>
  <div class="container my-4">
    
    <!-- Header -->
    <div class="text-center">
      <!-- LOGO & TITLE SIDE-BY-SIDE -->
      <div class="title-header">
        <div class="logo-container">
          <svg width="200" height="200" viewBox="0 0 200 200" xmlns="http://www.w3.org/2000/svg">
            <defs>
              <clipPath id="face-clip-b">
                <path d="M 100,20 C 60,20 30,50 30,100 C 30,170 70,190 100,190 C 130,190 170,170 170,100 C 170,50 140,20 100,20 Z" />
              </clipPath>
            </defs>
            <g clip-path="url(#face-clip-b)">
              <polygon points="30,20 100,100 30,120" fill="#003f5c"/>
              <polygon points="30,20 100,20 100,100" fill="#2f4b7c"/>
              <polygon points="100,20 170,20 100,100" fill="#665191"/>
              <polygon points="170,20 170,120 100,100" fill="#a05195"/>
              <polygon points="30,120 100,100 100,190 30,190" fill="#d45087"/>
              <polygon points="100,100 170,120 170,190 100,190" fill="#f95d6a"/>
              <polygon points="100,100 100,190 130,150" fill="#ff7c43"/>
              <polygon points="100,100 70,150 100,190" fill="#ffa600"/>
            </g>
            <circle cx="75" cy="90" r="12" fill="#ffffff"/>
            <polygon points="71,84 82,90 71,96" fill="#003f5c"/>
            <circle cx="125" cy="90" r="8" fill="#ffffff" />
            <circle cx="125"cy="90" r="4" fill="#003f5c" />
            <path d="M 90,140 Q 100,150 110,140" stroke="#ffffff" stroke-width="4" fill="none" stroke-linecap="round"/>
          </svg>
        </div>
        <div class="title-text-block">
          <h1>DH-OmniFace: A Comprehensive Dataset Suite for Controllable Human-Centric Video Generation</h1>
          <h3 class="publication-venue">Arxiv 2025</h3>
        </div>
      </div>
      
      <div class="author-line">
        <span class="author-block"><a href="https://github.com/fenghe12">He Feng</a><sup>1</sup>,</span>
        <span class="author-block"><a href="https://scholar.google.com/citations?hl=zh-CN&user=L8tcNioAAAAJ">Donglin Di</a><sup>1</sup>,</span>
        <span class="author-block"><a href="https://scholar.google.hk/citations?hl=zh-CN&user=67fxVzoAAAAJ">Tonghua Su</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Yin Chen</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Xiu Su</a><sup>2</sup>,</span>
        <span class="author-block"><a href="#">Hongyan Xu</a><sup>2</sup>,</span>
        <span class="author-block"><a href="#">Zhongjie Wang</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Xiangqian Wu</a><sup>1</sup>,</span>
        <span class="author-block"><a href="#">Song Yang</a><sup>3</sup>,</span>
        <span class="author-block"><a href="https://hellodfan.github.io/">Lei Fan</a><sup>3</sup></span>
      </div>
      
      <div class="institution-line mt-2">
        <span class="inst-block"><sup>1</sup>Harbin Institute of Technology</span>
        <span class="inst-block"><sup>2</sup>Central South University</span>
        <span class="inst-block"><sup>3</sup>University of New South Wales</span>
      </div>
      
      <div class="mt-4">
        <a class="btn btn-primary btn-lg m-2" href="#" role="button">Paper</a>
        <a class="btn btn-secondary btn-lg m-2" href="javascript:void(0);" role="button">Video</a>
        <a class="btn btn-secondary btn-lg m-2" href="javascript:void(0);" role="button">Poster</a>
        <a class="btn btn-success btn-lg m-2" href="https://docs.google.com/forms/d/e/1FAIpQLSeSMkATTXO22YvLFj-qC_hDM8LCd912Y45cdyINU91HbgP9KQ/viewform?usp=header" role="button">Data</a>
      </div>

    <!-- 1. MERGED Abstract & Overview Section -->
    <div class="my-5">
      <h2 class="section-title">📜 Overview & Abstract</h2>
      <img src="static/images/figure1-0918.png" class="img-fluid rounded figure-img" alt="Overview of the DH-OmniFace dataset">
      <p class="section-text mt-3">
         Overview of the <strong>DH-OmniFace</strong>. The suite includes seven components: <strong>DH-FaceVid-1K</strong> for talking face generation, <strong>DH-FaceEmoVid</strong> for expressive emotions, <strong>DH-FaceReliVid</strong> for diverse lighting conditions, <strong>DH-FaceLolVid</strong> for authentic laughter, <strong>DH-FaceDrasMvVid</strong> for large-angle head movements, <strong>DH-MVHeadVid</strong> for multi-view head poses, and <strong>DH-SingleVid</strong> for complex body movements.  
        The lower panels summarize our experiments: (i) fine-tuning generative backbones, (ii) introducing the FACET benchmark, and (iii) exploring joint training of face and full-body data in full-body video generation.
      </p>
      <p class="abstract-text section-text mt-4">
        Human-centric video generation has made significant progress in synthesizing realistic, temporally coherent videos. However, advancing controllable generation of fine-grained facial attributes, <em>e.g.</em>, emotion, head pose, and lighting conditions, is hindered by the lack of large-scale, high-quality datasets with rich annotations.  
        To address this gap, we present <strong>DH-OmniFace</strong>, a comprehensive suite of human-centric video datasets covering diverse facial attributes and full-body actions.  
        The suite comprises <strong>340,407</strong> video clips totaling <strong>1,879 hours</strong> from over <strong>20,000</strong> individuals, including <strong>128,392</strong> clips at <code>1080×1080</code> and <strong>23,887</strong> clips at 1080p.  
        To ensure quality, we establish a multi-stage pipeline to filter out noisy samples, such as hands appearing at frame edges and truncated logos, which could otherwise degrade generation quality.  
        Each sample is paired with synchronized speech audio and text annotations to support multimodal learning.  
        The dataset offers extensive attribute coverage, including six core emotions (<em>e.g.</em>, happy, angry, disgusted), 16 lighting conditions (artificial and natural light at different times of day), 360° head poses (captured by a 12-camera array covering frontal, profile, and back views), and dynamic actions (including laughter and complex body movements such as calisthenics and yoga).  
      </p>
      <div class="tldr">
        <strong>TL;DR:</strong> We introduce <strong>DH-OmniFace</strong>, a large-scale suite of human-centric video datasets with diverse facial attributes and full-body actions, enabling controllable and high-quality video generation across multiple modalities.
      </div>
    </div>
    
    <div class="my-5">
      <div class="text-center">
        <h2 class="title">📥 Download DH-OmniFace</h2>
        <a href="https://docs.google.com/forms/d/e/1FAIpQLSeSMkATTXO22YvLFj-qC_hDM8LCd912Y45cdyINU91HbgP9KQ/viewform?usp=header" class="btn-download-specific">Click me for application</a>
        <div class="description mt-2"><strong>Scale:</strong> 340k samples / 1.8k hrs duration / ~5 TB</div>
      </div>
    
      <div class="download-alert mt-4">
        <h3>🔥 Download Instructions & Policy</h3>
        <p>To download the <strong>DH-OmniFace</strong> dataset suite, please submit the <a href="https://docs.google.com/forms/d/e/1FAIpQLSeSMkATTXO22YvLFj-qC_hDM8LCd912Y45cdyINU91HbgP9KQ/viewform?usp=header"><strong>request form</strong></a> using an official institutional email and clearly stating your research purpose. Requests from personal emails will be rejected.</p>
        <p>After approval (usually within <strong>2–3 working days</strong>), download instructions will be sent via email. If you encounter any issues, contact <a href="mailto:fenghe021209@gmail.com"><strong>fenghe021209@gmail.com</strong></a>.</p>
        <p>These samples are sourced from crowd-sourcing platforms. By downloading, you agree to comply with <a href="https://github.com/fenghe12/DH-OmniFace/blob/main/LICENSE"><strong>the license agreement</strong></a>.</p>
      </div>
    </div>
    
    <!-- 3. Open-source Plan -->
<div class="my-5">
  <h2 class="section-title">🚀 Open-source Plan</h2>
  <p class="text-center">Our open-source roadmap is ongoing. We will update the status here as we make progress.</p>
  <div class="timeline mt-5">
    <div class="timeline-item status-current">
      <div class="timeline-dot">🔄</div>
      <div class="timeline-content">
        <h3>Phase 1 <span class="timeline-status">Ongoing</span></h3>
        <p>Open the DH-OmniFace dataset suite in a controlled manner for public access.</p>
      </div>
    </div>
  </div>
</div

## 🎬 Video Samples
### The videos have been cropped and compressed to ensure faster page loading, but this does not reflect the original quality.

<!-- 1. DH-FaceVid-1K -->
<h3 class="section-subtitle">DH-FaceVid-1K</h3>
<div id="facevid-gallery" class="video-gallery mt-3"></div>

<!-- 2. DH-FaceEmoVid -->
<h3 class="section-subtitle">DH-FaceEmoVid</h3>
<p class="text-center">The emotions include <b>happy, angry, fearful, disgusted, sad, and surprised</b>.</p>
<div id="faceemovid-gallery" class="video-gallery mt-3"></div>

<!-- 3. DH-FaceReliVid -->
<h3 class="section-subtitle">DH-FaceReliVid</h3>
<div id="facerelivid-gallery" class="video-gallery mt-3"></div>

<!-- 4. DH-FaceLolVid -->
<h3 class="section-subtitle">DH-FaceLolVid</h3>
<div id="facelolvid-gallery" class="video-gallery mt-3"></div>

<!-- 5. DH-FaceDrasMvVid -->
<h3 class="section-subtitle">DH-FaceDrasMvVid</h3>
<div id="facedrasmvvid-gallery" class="video-gallery mt-3"></div>

<!-- 6. DH-MVHeadVid -->
<h3 class="section-subtitle">DH-MVHeadVid</h3>
<div id="mvfacevid-gallery" class="video-gallery mt-3"></div>

<!-- 7. DH-SingleVid -->
<h3 class="section-subtitle">DH-SingleVid</h3>
<div id="singlevid-gallery" class="video-gallery mt-3"></div>


<!-- Statistics and Other Figures -->
<div class="my-5 text-center">
  <h2 class="section-title">📊 Datasets Comparison</h2>
  <p class="section-text">
    Compared with other datasets, <strong>DH-OmniFace</strong> has a larger data volume, superior visual quality, and broader attribute coverage.
  </p>
  <img src="static\images\img_v3_02r4_cebe0894-d5db-4059-80e3-0f1bb1ba3e9g.jpg" class="img-fluid rounded figure-img" alt="Comparison with other datasets">
</div>

<div class="my-5 text-center">
  <h2 class="section-title">📈 Statistics</h2>
  <p class="section-text">
    Details of each subset.
  </p>
  <img src="static\images\img_v3_02r4_88423c55-6652-4cf9-89b0-a19261c1336g.jpg" class="img-fluid rounded figure-img" alt="Dataset statistics">
</div>



<div class="my-5 text-center">
  <h2 class="section-title">⚙️ Collection Pipeline</h2>
  <img src="static/images/figure4_cropped.png" class="img-fluid rounded figure-img" alt="Data collection pipeline">
  <p class="section-text mt-3">
    Overview of the data curation pipeline, comprising four main stages: raw data acquisition, video preprocessing, noisy data filtering, and annotation generation using Qwen2.5-VL and Llama 3, followed by two-stage manual verification.
  </p>
</div>

<!-- BibTeX -->
<div class="my-5">
  <h2 class="section-title">📚 BibTeX</h2>
  Coming Soon......
  <!-- <div class="bibtex-box">
    <pre><code>@inproceedings{di2025facevid,
title = {DH-FaceVid-1K: A Large-Scale High-Quality Dataset for Face Video Generation},
author = {Di, Donglin and Feng, He and Sun, Wenzhang and Ma, Yongjia and Li, Hao and Chen, Wei and Fan, Lei and Su, Tonghua and Yang, Xun},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
year = {2025}
}</code></pre> -->
  </div>
</div>

<!-- Footer -->
<footer class="text-center my-5">
  <p>Homepage Template adapted from <a href="https://github.com/m-niemeyer/regnerf" target="_blank">the RegNeRF project page</a>.</p>
</footer>

</div>

<!-- Consolidated JavaScript -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  function createVideoGrid(containerId, fileList, directory) {
    const gridContainer = document.getElementById(containerId);
    if (!gridContainer) return;

    fileList.forEach(item => {
      // Allow for both simple strings and objects with different directories
      const filename = typeof item === 'string' ? item : item.filename;
      const itemDirectory = typeof item === 'string' ? directory : item.directory;

      const wrapper = document.createElement('div');
      wrapper.className = 'video-wrapper';

      const video = document.createElement('video');
      video.loop = true;
      video.muted = true;
      video.playsInline = true;
      video.preload = 'metadata';
      
      wrapper.addEventListener('mouseenter', () => video.play().catch(e => {}));
      wrapper.addEventListener('mouseleave', () => video.pause());

      const source = document.createElement('source');
      source.src = itemDirectory + filename;
      source.type = 'video/mp4';

      const title = document.createElement('div');
      title.className = 'video-title';
      title.textContent = filename.replace('.mp4', '');

      video.appendChild(source);
      wrapper.appendChild(video);
      wrapper.appendChild(title);
      gridContainer.appendChild(wrapper);
      
      wrapper.addEventListener('click', () => {
        if (video.paused) video.play().catch(e => {});
        else video.pause();
      });
    });
  }

  // --- Data for Each Video Section ---

  // **MODIFIED as per your request**
  const faceVidFiles = [
    { filename: "001106.mp4", directory: "facevid/age/" },
    { filename: "002148.mp4", directory: "facevid/age/" },
    { filename: "003762.mp4", directory: "facevid/age/" },
    { filename: "019479.mp4", directory: "facevid/age/" },
    { filename: "BFs-a-hqs2I_9.mp4", directory: "facevid/race/" },
    { filename: "3lfO6OCqcCA_0.mp4", directory: "facevid/race/" },
    { filename: "Uu3xazfdmvk_34.mp4", directory: "facevid/race/" },
    { filename: "WN2XSI6vZIg_18.mp4", directory: "facevid/race/" }
  ];
  
  // **CONFIRMED as per your request**
  const singleVidFiles = ["dance-pose1.mp4", "dance-pose2.mp4", "gymnastic-pose2.mp4", "scene-pose1.mp4", "yoga-pose1-1.mp4", "yoga-pose1.mp4"];
  
  // Unchanged sections
  const faceEmoVidFiles = ["001_happy_038.mp4", "021_happy_030.mp4", "002_angry_001.mp4", "003_angry_037.mp4", "005_fear_003.mp4", "018_fear_51.mp4", "006_disgust_031.mp4", "021_disgust_023.mp4", "018_sad_036.mp4", "045_sad_011.mp4", "044_surprise_010.mp4", "046_surprise_038.mp4"];
  const faceReliVidFiles = ["0001_light1_0001.mp4", "0001_light2_0001.mp4", "0001_light5_0005.mp4", "0001_light6_0004.mp4", "0002_light2_0020.mp4", "0003_light1_0011.mp4", "0003_light8_0013.mp4", "0011_light2_0008.mp4", "0014_light3_0012.mp4", "0070_light2_0010.mp4", "outdoor_0006_light6_0009.mp4", "outdoor_0006_light8_0004.mp4", "outdoor_0008_light2_0011.mp4", "outdoor_0009_light3_0006.mp4", "outdoor_0041_light3_0017.mp4", "outdoor_0042_light1_0010.mp4", "outdoor_0051_light3_0008.mp4", "outdoor_0052_light4_0012.mp4"];
  const faceLolVidFiles = ["2024_09_05_14_51_IMG_4368.mp4", "VID_20240909_103040.mp4", "VID_20240912_134857.mp4", "VID20240909085206.mp4", "VID20240920152148.mp4", "VID20240922151142.mp4"];
  const faceDrasMvVidFiles = ["batch_04_004_VID20240823094316.mp4", "batch_04_002_VID20240822115129.mp4", "batch_03_034_VID20240821130848.mp4", "batch_03_033_VID20240821102408.mp4", "batch_02_030_VID20240820142103.mp4", "batch_01_026_VID_20240812084921.mp4"];
  const mvFaceVidFiles = ["001-张芳芳-女_cam3_VID_20240920_113946.mp4", "004-李子豪-男_cam4_VID_20240926_222900.mp4", "004-李子豪-男_cam4_VID_20240926_223654.mp4", "005-王志胜-男_cam2_VID_20240927_194023.mp4", "006-王晓雅-女_cam7_VID_20240924_211248.mp4", "007-李晓光-男_cam2_VID_20240928_191226.mp4", "007-李晓光-男_cam5_VID_20240928_191828.mp4", "008-王泽-男_cam4_VID_20240929_162344.mp4", "008-王泽-男_cam6_VID_20240929_160811.mp4", "009-张嫣然-女_cam4_VID_20240924_174540.mp4", "010-李鹏飞-男_cam5_VID_20240930_224028.mp4", "010-李鹏飞-男_cam8_VID_20240930_225926.mp4"];
  
  // --- Create Grids for Each Section ---
  createVideoGrid('facevid-gallery', faceVidFiles, ""); // Directory info is now in the array itself
  createVideoGrid('faceemovid-gallery', faceEmoVidFiles, "dh-faceemovid/");
  createVideoGrid('facerelivid-gallery', faceReliVidFiles, "subsets/DH-FaceReliVid/");
  createVideoGrid('facelolvid-gallery', faceLolVidFiles, "subsets/DH-FaceLolVid/");
  createVideoGrid('facedrasmvvid-gallery', faceDrasMvVidFiles, "subsets/DH-FaceDrasMvVid/");
  createVideoGrid('mvfacevid-gallery', mvFaceVidFiles, "subsets/DH-MVFaceVid/");
  createVideoGrid('singlevid-gallery', singleVidFiles, "subsets/DH-SingleVid/");
});
</script>
</body>
</html>